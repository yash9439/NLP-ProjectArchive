{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install conllu\n\nimport numpy as np \nimport pandas as pd \nimport conllu\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-09T10:24:43.845998Z","iopub.execute_input":"2023-03-09T10:24:43.846413Z","iopub.status.idle":"2023-03-09T10:24:58.802049Z","shell.execute_reply.started":"2023-03-09T10:24:43.846377Z","shell.execute_reply":"2023-03-09T10:24:58.800438Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting conllu\n  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\nInstalling collected packages: conllu\nSuccessfully installed conllu-4.5.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\n/kaggle/input/googlewordtovec/GoogleNews-vectors-negative300.bin\n/kaggle/input/savedmodel-postag/savedModel.pth\n/kaggle/input/pos-tag/en_atis-ud-train.conllu\n/kaggle/input/pos-tag/en_atis-ud-test.conllu\n/kaggle/input/pos-tag/en_atis-ud-dev.conllu\n/kaggle/input/glove6b300dtxt/glove.6B.300d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:24:58.805568Z","iopub.execute_input":"2023-03-09T10:24:58.806135Z","iopub.status.idle":"2023-03-09T10:25:01.545677Z","shell.execute_reply.started":"2023-03-09T10:24:58.806075Z","shell.execute_reply":"2023-03-09T10:25:01.544568Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Loading dataset\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport math\n\n\nclass TreeBankDataset(Dataset):\n    def __init__(self, filepath):\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            data = f.read()\n        parsed_data = conllu.parse(data)\n        self.data = []\n        for sentence in parsed_data:\n            tagged_sentence_word = []\n            tagged_sentence_tag = []\n            for token in sentence:\n                word = token[\"form\"]\n                pos = token[\"upos\"]\n                tagged_sentence_word.append(word)\n                tagged_sentence_tag.append(pos)\n            self.data.append((tagged_sentence_word,tagged_sentence_tag))\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:01.546903Z","iopub.execute_input":"2023-03-09T10:25:01.548279Z","iopub.status.idle":"2023-03-09T10:25:01.990308Z","shell.execute_reply.started":"2023-03-09T10:25:01.548236Z","shell.execute_reply":"2023-03-09T10:25:01.988702Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset_train = TreeBankDataset(\"/kaggle/input/pos-tag/en_atis-ud-train.conllu\")\ndataset_test = TreeBankDataset(\"/kaggle/input/pos-tag/en_atis-ud-test.conllu\")\ndataset_dev = TreeBankDataset(\"/kaggle/input/pos-tag/en_atis-ud-dev.conllu\")","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:01.993174Z","iopub.execute_input":"2023-03-09T10:25:01.993552Z","iopub.status.idle":"2023-03-09T10:25:03.833181Z","shell.execute_reply.started":"2023-03-09T10:25:01.993516Z","shell.execute_reply":"2023-03-09T10:25:03.831929Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# training sentences and their corresponding word-tags\ntraining_data = dataset_train\n\n# create a dictionary that maps words to indices\nword2idx = {}\ntag2idx = {}\nidx2tag = {}\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word2idx:\n            word2idx[word] = len(word2idx)\n\nfor sent, tags in training_data:\n    for tag in tags:\n        if tag not in tag2idx:\n            tag2idx[tag] = len(tag2idx)\n            idx2tag[len(idx2tag)] = tag\n\nword2idx[\"UNK\"] = len(word2idx)\ntag2idx['UNK'] = len(tag2idx)\nidx2tag[len(idx2tag)] = 'UNK'","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:03.834926Z","iopub.execute_input":"2023-03-09T10:25:03.836118Z","iopub.status.idle":"2023-03-09T10:25:03.871390Z","shell.execute_reply.started":"2023-03-09T10:25:03.836062Z","shell.execute_reply":"2023-03-09T10:25:03.869512Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(training_data[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:03.873432Z","iopub.execute_input":"2023-03-09T10:25:03.874323Z","iopub.status.idle":"2023-03-09T10:25:03.881898Z","shell.execute_reply.started":"2023-03-09T10:25:03.874266Z","shell.execute_reply":"2023-03-09T10:25:03.880330Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(['what', 'is', 'the', 'cost', 'of', 'a', 'round', 'trip', 'flight', 'from', 'pittsburgh', 'to', 'atlanta', 'beginning', 'on', 'april', 'twenty', 'fifth', 'and', 'returning', 'on', 'may', 'sixth'], ['PRON', 'AUX', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'VERB', 'ADP', 'NOUN', 'NUM', 'ADJ', 'CCONJ', 'VERB', 'ADP', 'NOUN', 'ADJ'])\n","output_type":"stream"}]},{"cell_type":"code","source":"# print out the created dictionary\nprint(word2idx)\nprint(tag2idx)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:03.884180Z","iopub.execute_input":"2023-03-09T10:25:03.884706Z","iopub.status.idle":"2023-03-09T10:25:03.895912Z","shell.execute_reply.started":"2023-03-09T10:25:03.884642Z","shell.execute_reply":"2023-03-09T10:25:03.894671Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'what': 0, 'is': 1, 'the': 2, 'cost': 3, 'of': 4, 'a': 5, 'round': 6, 'trip': 7, 'flight': 8, 'from': 9, 'pittsburgh': 10, 'to': 11, 'atlanta': 12, 'beginning': 13, 'on': 14, 'april': 15, 'twenty': 16, 'fifth': 17, 'and': 18, 'returning': 19, 'may': 20, 'sixth': 21, 'now': 22, 'i': 23, 'need': 24, 'leaving': 25, 'fort': 26, 'worth': 27, 'arriving': 28, 'in': 29, 'denver': 30, 'no': 31, 'later': 32, 'than': 33, '2': 34, 'pm': 35, 'next': 36, 'monday': 37, 'fly': 38, 'kansas': 39, 'city': 40, 'chicago': 41, 'wednesday': 42, 'following': 43, 'day': 44, 'meaning': 45, 'meal': 46, 'code': 47, 's': 48, 'show': 49, 'me': 50, 'all': 51, 'flights': 52, 'which': 53, 'serve': 54, 'for': 55, 'after': 56, 'tomorrow': 57, 'us': 58, 'air': 59, 'list': 60, 'nonstop': 61, 'early': 62, 'tuesday': 63, 'morning': 64, 'dallas': 65, 'st.': 66, 'petersburg': 67, 'toronto': 68, 'that': 69, 'arrive': 70, 'listing': 71, 'new': 72, 'york': 73, 'montreal': 74, 'canada': 75, 'departing': 76, 'thursday': 77, 'american': 78, 'airlines': 79, 'ontario': 80, 'with': 81, 'stopover': 82, 'louis': 83, 'ground': 84, 'transportation': 85, 'houston': 86, 'afternoon': 87, 'schedule': 88, 'philadelphia': 89, 'san': 90, 'francisco': 91, 'evening': 92, 'diego': 93, 'layover': 94, 'washington': 95, 'dc': 96, 'are': 97, 'there': 98, 'any': 99, 'boston': 100, 'stop': 101, 'restrictions': 102, 'cheapest': 103, 'one': 104, 'way': 105, 'fare': 106, 'between': 107, 'oakland': 108, 'airfare': 109, '416': 110, 'dollars': 111, \"'s\": 112, 'restriction': 113, 'ap68': 114, 'california': 115, 'airports': 116, 'available': 117, 'texas': 118, 'airport': 119, 'closest': 120, 'nevada': 121, 'arizona': 122, 'actually': 123, 'las': 124, 'vegas': 125, 'burbank': 126, 'saturday': 127, 'two': 128, 'how': 129, 'many': 130, 'going': 131, 'july': 132, 'seventh': 133, 'would': 134, 'like': 135, 'an': 136, 'february': 137, 'eighth': 138, 'before': 139, '9': 140, 'am': 141, 'second': 142, 'late': 143, 'okay': 144, 'june': 145, 'first': 146, \"'d\": 147, 'go': 148, 'phoenix': 149, 'detroit': 150, 'milwaukee': 151, 'indianapolis': 152, 'does': 153, 'ls': 154, 'stand': 155, 'designate': 156, 'as': 157, 'baltimore': 158, '1115': 159, '1245': 160, 'miami': 161, 'daily': 162, '8': 163, 'trans': 164, 'world': 165, 'airline': 166, '1030': 167, '1130': 168, '5': 169, '730': 170, 'leave': 171, 'charlotte': 172, 'north': 173, 'carolina': 174, '4': 175, 'find': 176, 'newark': 177, 'jersey': 178, 'cleveland': 179, 'ohio': 180, 'do': 181, 'you': 182, 'have': 183, 'connect': 184, 'international': 185, 'minneapolis': 186, 'rental': 187, 'cars': 188, \"'ll\": 189, 'rent': 190, 'car': 191, 'sort': 192, 'near': 193, 'fine': 194, 'can': 195, 'give': 196, 'information': 197, 'downtown': 198, 'economy': 199, 'class': 200, 'fares': 201, 'december': 202, 'sixteenth': 203, 'codes': 204, 'belong': 205, 'coach': 206, 'night': 207, 'service': 208, 'november': 209, 'twelfth': 210, 'eleventh': 211, 'want': 212, 'know': 213, 'or': 214, '1': 215, \"o'clock\": 216, '3': 217, '6': 218, '10': 219, 'august': 220, 'display': 221, 'depart': 222, 'please': 223, 'snacks': 224, 'served': 225, 'tower': 226, 'types': 227, 'meals': 228, 'ever': 229, 'my': 230, 'options': 231, '382': 232, 'get': 233, 'bwi': 234, 'eastern': 235, '210': 236, 'delta': 237, '852': 238, 'latest': 239, 'return': 240, 'same': 241, 'back': 242, 'most': 243, 'hours': 244, 'take': 245, 'so': 246, 'when': 247, 'will': 248, 'maximum': 249, 'amount': 250, 'time': 251, 'still': 252, 'earliest': 253, 'departure': 254, 'be': 255, 'travel': 256, 'at': 257, 'around': 258, '7': 259, 'route': 260, 'lastest': 261, 'longest': 262, 'but': 263, 'possible': 264, 'only': 265, 'weekdays': 266, 'red': 267, 'eye': 268, 'los': 269, 'angeles': 270, 'ten': 271, 'people': 272, 'during': 273, 'week': 274, 'days': 275, 'out': 276, 'arrives': 277, 'salt': 278, 'lake': 279, 'cincinnati': 280, 'area': 281, 'explain': 282, 'ap': 283, '57': 284, '20': 285, 'mean': 286, '80': 287, 'twa': 288, '497766': 289, 'has': 290, 'stops': 291, 'friday': 292, '705': 293, 'number': 294, 'book': 295, 'least': 296, '813': 297, 'goes': 298, 'straight': 299, 'through': 300, 'without': 301, 'stopping': 302, 'another': 303, 'florida': 304, 'tell': 305, 'about': 306, 'by': 307, 'memphis': 308, 'tennessee': 309, 'noon': 310, '530': 311, 'off': 312, 'love': 313, 'field': 314, 'united': 315, 'la': 316, 'guardia': 317, 'jfk': 318, 'mco': 319, 'sfo': 320, '1991': 321, 'orlando': 322, 'lowest': 323, 'dfw': 324, 'ticket': 325, 'oak': 326, 'atl': 327, 'logan': 328, 'march': 329, 'numbers': 330, 'expensive': 331, 'continental': 332, 'leaves': 333, '1220': 334, 'seattle': 335, 'columbus': 336, 'minnesota': 337, 'those': 338, 'via': 339, 'rentals': 340, 'sunday': 341, 'rates': 342, 'costs': 343, 'limousine': 344, 'taxi': 345, 'operation': 346, 'ap80': 347, 'ap57': 348, 'ninth': 349, '12': 350, 'america': 351, 'west': 352, 'could': 353, 'fifteenth': 354, 'serves': 355, 'dinner': 356, 'provided': 357, 'cities': 358, 'where': 359, 'lester': 360, 'pearson': 361, 'canadian': 362, 'other': 363, 'earlier': 364, '1017': 365, 'northwest': 366, 'general': 367, 'mitchell': 368, 'located': 369, 'both': 370, 'nationair': 371, 'midwest': 372, 'express': 373, 'flies': 374, 'zone': 375, 'flying': 376, 'into': 377, 'much': 378, 'price': 379, 'it': 380, 'tacoma': 381, 'anywhere': 382, '1850': 383, 'midnight': 384, 'january': 385, '1992': 386, 'not': 387, 'exceeding': 388, '300': 389, 'tenth': 390, '1993': 391, '1505': 392, 'october': 393, '1994': 394, 'carries': 395, 'smallest': 396, 'passengers': 397, 'thirty': 398, 'third': 399, 'arrival': 400, 'schedules': 401, 'times': 402, 'your': 403, '269': 404, '428': 405, 'westchester': 406, 'county': 407, 'right': 408, 'september': 409, 'twentieth': 410, 'f28': 411, '755': 412, 'nights': 413, 'their': 414, 'prices': 415, '1039': 416, 'less': 417, '1100': 418, 'nashville': 419, 'again': 420, 'repeat': 421, 'make': 422, 'iah': 423, 'ord': 424, 'ewr': 425, 'dca': 426, 'cvg': 427, 'bna': 428, 'mci': 429, 'hou': 430, 'lga': 431, 'lax': 432, 'yyz': 433, 'bur': 434, 'long': 435, 'distance': 436, 'far': 437, 'paul': 438, 'miles': 439, 'name': 440, 'serviced': 441, 'regarding': 442, 'tampa': 443, 'names': 444, 'describe': 445, 'nineteenth': 446, 'seating': 447, 'capacity': 448, 'fourteenth': 449, 'aircraft': 450, 'largest': 451, 'plane': 452, 'eight': 453, 'sixteen': 454, 'departures': 455, 'seventeenth': 456, 'arrivals': 457, 'type': 458, 'greatest': 459, 'more': 460, 'business': 461, 'total': 462, 'instead': 463, 'besides': 464, 'turboprop': 465, '1059': 466, 'advertises': 467, 'having': 468, 'land': 469, 'various': 470, 'dulles': 471, 'boeing': 472, '767': 473, '466': 474, '329': 475, 'under': 476, '932': 477, '1000': 478, '200': 479, '124': 480, 'along': 481, 'equal': 482, '150': 483, 'each': 484, '400': 485, 'fit': 486, '72s': 487, 'airplane': 488, 'l1011': 489, 'hold': 490, '733': 491, 'airplanes': 492, 'uses': 493, '73s': 494, 'seats': 495, '734': 496, 'm80': 497, 'l10': 498, 'carried': 499, 'capacities': 500, '757': 501, 'planes': 502, 'd9s': 503, '100': 504, 'thrift': 505, 'level': 506, 'see': 507, 'thirtieth': 508, '505': 509, '163': 510, 'tonight': 511, 'connecting': 512, 'also': 513, 'making': 514, \"'m\": 515, 'looking': 516, 'hopefully': 517, 'makes': 518, 'yes': 519, 'breakfast': 520, 'direct': 521, 'itinerary': 522, 'departs': 523, '1940': 524, 'connects': 525, 'provide': 526, 'used': 527, 'including': 528, 'connections': 529, 'if': 530, 'either': 531, 'preferably': 532, 'local': 533, 'beach': 534, 'then': 535, 'mornings': 536, 'four': 537, 'combination': 538, 'thank': 539, 'using': 540, 'well': 541, 'run': 542, 'colorado': 543, 'fourth': 544, 'who': 545, 'sure': 546, 'determine': 547, 'use': 548, '1765': 549, 'lufthansa': 550, 'eighteenth': 551, 'f': 552, 'today': 553, 'come': 554, '320': 555, 'booking': 556, 'k': 557, 'classes': 558, 'yn': 559, 'j31': 560, 'different': 561, 'dh8': 562, 'minimum': 563, 'connection': 564, 'intercontinental': 565, 'last': 566, 'aa': 567, '459': 568, 'limousines': 569, 'services': 570, 'jose': 571, 'too': 572, 'train': 573, 'stapleton': 574, 'limo': 575, 'georgia': 576, 'pennsylvania': 577, 'utah': 578, 'missouri': 579, 'interested': 580, 'shortest': 581, 'quebec': 582, 'michigan': 583, 'indiana': 584, 'this': 585, 'wednesdays': 586, '82': 587, '139': 588, 'tickets': 589, 'sounds': 590, 'great': 591, 'let': 592, 'takeoffs': 593, 'landings': 594, 'grounds': 595, 'offer': 596, 'transport': 597, 'kind': 598, 'hi': 599, 'calling': 600, 'coming': 601, 'soon': 602, 'thereafter': 603, 'anything': 604, 'bring': 605, 'up': 606, 'y': 607, 'm': 608, 'difference': 609, 'q': 610, 'b': 611, 'qo': 612, 'qw': 613, 'qx': 614, 'fn': 615, 'qualify': 616, 'h': 617, 'basis': 618, 'bh': 619, 'offers': 620, 'included': 621, 'serving': 622, 'trying': 623, 'include': 624, 'whether': 625, 'offered': 626, 'ua': 627, '270': 628, 'being': 629, '747': 630, 'be1': 631, '737': 632, 'very': 633, 'working': 634, 'scenario': 635, 'three': 636, '727': 637, 'called': 638, 'dc10': 639, 'abbreviation': 640, 'd10': 641, 'includes': 642, '296': 643, 'should': 644, 'lunch': 645, '343': 646, 'travels': 647, 'snack': 648, 'supper': 649, '838': 650, '1110': 651, 'reaches': 652, 'sometime': 653, 'some': 654, 'reaching': 655, 'saturdays': 656, 'vicinity': 657, 'good': 658, '1800': 659, 'overnight': 660, 'final': 661, 'destination': 662, 'over': 663, 'summer': 664, '297': 665, '1222': 666, '281': 667, 'listed': 668, 'dl': 669, '1055': 670, '405': 671, '201': 672, '315': 673, '21': 674, '486': 675, '825': 676, '555': 677, '1207': 678, '1500': 679, '639': 680, '217': 681, '71': 682, '106': 683, '539': 684, '3724': 685, '271': 686, '1291': 687, '4400': 688, '3357': 689, '345': 690, '771': 691, 'co': 692, '1209': 693, 'ea': 694, '212': 695, '257': 696, '608': 697, '746': 698, 'taking': 699, '311': 700, '417': 701, 'try': 702, 'inform': 703, 'kinds': 704, 'traveling': 705, '419': 706, 'they': 707, 'these': 708, 'kindly': 709, 'proper': 710, 'town': 711, 'takeoff': 712, 'kennedy': 713, 'close': 714, '230': 715, 'nonstops': 716, 'thursdays': 717, \"'re\": 718, '1230': 719, '1200': 720, 'within': 721, 'reservation': 722, 'friends': 723, 'visit': 724, 'here': 725, 'them': 726, 'lives': 727, '0900': 728, '1600': 729, '11': 730, 'we': 731, 'nighttime': 732, 'southwest': 733, 'usa': 734, 'able': 735, 'put': 736, '630': 737, 'nw': 738, 'hp': 739, 'define': 740, 'ff': 741, 'symbols': 742, 'stands': 743, 'kw': 744, 'sam': 745, 'ac': 746, '718': 747, 'wn': 748, 'arrangements': 749, 'sorry': 750, 'must': 751, 'originating': 752, '225': 753, '1158': 754, 'equipment': 755, 'choices': 756, '1205': 757, '1145': 758, 'abbreviations': 759, 'jet': 760, 'companies': 761, 'continuing': 762, 'represented': 763, 'database': 764, 'single': 765, 'rate': 766, 'trips': 767, 'stopovers': 768, 'directly': 769, 'starting': 770, 'afterwards': 771, 'reservations': 772, 'scheduled': 773, 'seat': 774, 'india': 775, 'buy': 776, 'six': 777, '1700': 778, 'say': 779, 'mealtime': 780, '2100': 781, 'economic': 782, 'wish': 783, 'discount': 784, 'staying': 785, 'while': 786, 'look': 787, 'across': 788, 'continent': 789, 'transcontinental': 790, 'begins': 791, 'lands': 792, 'landing': 793, 'month': 794, 'help': 795, '720': 796, '110': 797, 'such': 798, '1045': 799, '934': 800, 'heading': 801, 'toward': 802, '430': 803, 'approximately': 804, '324': 805, '1300': 806, '723': 807, '1020': 808, '645': 809, 'weekday': 810, 'inexpensive': 811, 'thing': 812, 'cheap': 813, 'thanks': 814, 'question': 815, 'live': 816, 'spend': 817, 'seventeen': 818, 'highest': 819, 'priced': 820, 'charges': 821, 'dinnertime': 822, '305': 823, '845': 824, 'noontime': 825, '1026': 826, '823': 827, '2134': 828, '1024': 829, '130': 830, \"'ve\": 831, 'got': 832, 'somebody': 833, 'else': 834, 'wants': 835, '420': 836, 'seven': 837, 'catch': 838, 'fifteen': 839, 'thirteenth': 840, 'tuesdays': 841, 'midway': 842, 'alaska': 843, 'arrange': 844, 'plan': 845, 'oh': 846, 'hello': 847, \"n't\": 848, 'prefer': 849, 'requesting': 850, 'comes': 851, 'mondays': 852, 'bound': 853, 'fridays': 854, 'sundays': 855, 'bay': 856, 'planning': 857, 'home': 858, 'reverse': 859, 'order': 860, 'a.m.': 861, 'philly': 862, 'UNK': 863}\n{'PRON': 0, 'AUX': 1, 'DET': 2, 'NOUN': 3, 'ADP': 4, 'PROPN': 5, 'VERB': 6, 'NUM': 7, 'ADJ': 8, 'CCONJ': 9, 'ADV': 10, 'PART': 11, 'INTJ': 12, 'UNK': 13}\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\ndef texts_to_sequences(text, word_index):\n    sequence = []\n    for token in text:\n        if token in word_index.keys():\n            sequence.append(word_index[token])\n        else:\n            sequence.append(word_index['UNK'])\n    sequence = np.array(sequence)\n    return torch.from_numpy(sequence)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:03.897264Z","iopub.execute_input":"2023-03-09T10:25:03.897663Z","iopub.status.idle":"2023-03-09T10:25:03.905769Z","shell.execute_reply.started":"2023-03-09T10:25:03.897628Z","shell.execute_reply":"2023-03-09T10:25:03.904876Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Checking texts_to_sequences\nexample_input = texts_to_sequences(\"i am going via flight\".lower().split(), word2idx)\n\nprint(example_input)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:03.907084Z","iopub.execute_input":"2023-03-09T10:25:03.911624Z","iopub.status.idle":"2023-03-09T10:25:03.962816Z","shell.execute_reply.started":"2023-03-09T10:25:03.911571Z","shell.execute_reply":"2023-03-09T10:25:03.961326Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([ 23, 141, 131, 339,   8])\n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\npath = '/kaggle/input/googlewordtovec/GoogleNews-vectors-negative300.bin'\n\nword2vec = KeyedVectors.load_word2vec_format(path, binary=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:25:03.968654Z","iopub.execute_input":"2023-03-09T10:25:03.969132Z","iopub.status.idle":"2023-03-09T10:26:16.595155Z","shell.execute_reply.started":"2023-03-09T10:25:03.969082Z","shell.execute_reply":"2023-03-09T10:26:16.593945Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_SIZE = 300\nVOCABULARY_SIZE = len(word2idx)\n\nembedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n\nfor word, index in word2idx.items():\n    try:\n        embedding_weights[index, :] = word2vec[word]\n    except KeyError:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.597153Z","iopub.execute_input":"2023-03-09T10:26:16.597989Z","iopub.status.idle":"2023-03-09T10:26:16.608985Z","shell.execute_reply.started":"2023-03-09T10:26:16.597949Z","shell.execute_reply":"2023-03-09T10:26:16.607890Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"embedding_weights = torch.from_numpy(embedding_weights)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.611145Z","iopub.execute_input":"2023-03-09T10:26:16.611679Z","iopub.status.idle":"2023-03-09T10:26:16.618570Z","shell.execute_reply.started":"2023-03-09T10:26:16.611629Z","shell.execute_reply":"2023-03-09T10:26:16.617230Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(embedding_weights)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.619722Z","iopub.execute_input":"2023-03-09T10:26:16.620590Z","iopub.status.idle":"2023-03-09T10:26:16.698272Z","shell.execute_reply.started":"2023-03-09T10:26:16.620467Z","shell.execute_reply":"2023-03-09T10:26:16.696873Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"tensor([[ 0.1396, -0.0062,  0.2148,  ...,  0.0571,  0.0996, -0.2344],\n        [ 0.0070, -0.0732,  0.1719,  ...,  0.0112,  0.1641,  0.1069],\n        [ 0.0801,  0.1050,  0.0498,  ...,  0.0037,  0.0476, -0.0688],\n        ...,\n        [-0.0820, -0.0098,  0.0142,  ..., -0.2021,  0.0713,  0.1621],\n        [-0.3809,  0.0444, -0.0811,  ..., -0.1963, -0.1396,  0.0311],\n        [-0.0947, -0.5117,  0.0581,  ...,  0.3906, -0.0034,  0.2812]],\n       dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"code","source":"class BiLSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_embeddings, dropout = 0.3):\n        ''' Initialize the layers of this model.'''\n        super(BiLSTMTagger, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n\n        # embedding layer that turns words into a vector of a specified size\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.word_embeddings.weight.data.copy_(pretrained_embeddings)\n        # the LSTM takes embedded word vectors (of a specified size) as inputs \n        # and outputs hidden states of size hidden_dim\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n        \n        # initialize the hidden state (see code below)\n        self.hidden = self.init_hidden()\n\n        \n    def init_hidden(self):\n        ''' At the start of training, we need to initialize a hidden state;\n           there will be none because the hidden state is formed based on perviously seen data.\n           So, this function defines a hidden state with all zeroes and of a specified size.'''\n        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n        return (torch.zeros(2, 1, self.hidden_dim),\n                torch.zeros(2, 1, self.hidden_dim))\n\n    def forward(self, sentence):\n        ''' Define the feedforward behavior of the model.'''\n        # create embedded word vectors for each word in a sentence\n        embeds = self.word_embeddings(sentence)\n        \n        # get the output and hidden state by passing the lstm over our word embeddings\n        # the lstm takes in our embeddings and hiddent state\n        lstm_out, self.hidden = self.lstm(\n            embeds.view(len(sentence), 1, -1), self.hidden)\n        \n        lstm_out = self.dropout(lstm_out)\n        \n        # get the scores for the most likely tag for a word\n        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_outputs, dim=1)\n        \n        return tag_scores","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.700787Z","iopub.execute_input":"2023-03-09T10:26:16.701318Z","iopub.status.idle":"2023-03-09T10:26:16.762202Z","shell.execute_reply.started":"2023-03-09T10:26:16.701261Z","shell.execute_reply":"2023-03-09T10:26:16.760942Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 300\nHIDDEN_DIM = 128\n\n# instantiate our model\nmodel = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx), embedding_weights)\n\n# define our loss and optimizer\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.763891Z","iopub.execute_input":"2023-03-09T10:26:16.764426Z","iopub.status.idle":"2023-03-09T10:26:16.798324Z","shell.execute_reply.started":"2023-03-09T10:26:16.764378Z","shell.execute_reply":"2023-03-09T10:26:16.796803Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_sentence = \"flights are available\".lower().split()\n\ninputs = texts_to_sequences(test_sentence, word2idx)\ninputs = inputs\ntag_scores = model(inputs)\nprint(tag_scores)\n\n_, predicted_tags = torch.max(tag_scores, 1)\nprint('\\n')\nprint('Predicted tags: \\n',predicted_tags)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.800928Z","iopub.execute_input":"2023-03-09T10:26:16.801426Z","iopub.status.idle":"2023-03-09T10:26:16.868907Z","shell.execute_reply.started":"2023-03-09T10:26:16.801385Z","shell.execute_reply":"2023-03-09T10:26:16.867360Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor([[-2.5577, -2.6538, -2.6706, -2.5615, -2.5932, -2.5871, -2.6825, -2.6387,\n         -2.7014, -2.6552, -2.6336, -2.7518, -2.6199, -2.6588],\n        [-2.5564, -2.6386, -2.6598, -2.5994, -2.6507, -2.6050, -2.6215, -2.6161,\n         -2.6925, -2.6549, -2.6401, -2.7116, -2.6525, -2.6578],\n        [-2.6104, -2.6700, -2.6919, -2.5759, -2.6912, -2.5863, -2.5629, -2.6649,\n         -2.7028, -2.6257, -2.6212, -2.7111, -2.6552, -2.5936]],\n       grad_fn=<LogSoftmaxBackward0>)\n\n\nPredicted tags: \n tensor([0, 0, 6])\n","output_type":"stream"}]},{"cell_type":"code","source":"PATH = '/kaggle/input/savedmodel-postag/savedModel.pth'\nmodel.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:26:16.870737Z","iopub.execute_input":"2023-03-09T10:26:16.871772Z","iopub.status.idle":"2023-03-09T10:26:16.908757Z","shell.execute_reply.started":"2023-03-09T10:26:16.871731Z","shell.execute_reply":"2023-03-09T10:26:16.907278Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_true = []\ny_pred = []\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\ntarget_names = ['PRON', 'AUX', 'DET', 'NOUN', 'ADP', 'PROPN', 'VERB', 'NUM', 'ADJ', 'CCONJ', 'ADV', 'PART', 'INTJ']\n\n\nepoch_total = 0\ntotal_loss = 0.0\ntotal_correct = 0\ntotal_total = 0\nepoch_correct = 0\nfor sentence, tags in dataset_train:\n    sentence_in = texts_to_sequences(sentence, word2idx)\n    targets = texts_to_sequences(tags, tag2idx)\n    tag_scores = model(sentence_in)\n    loss = loss_function(tag_scores, targets)\n    total_loss += loss\n    _, predicted_tags = torch.max(tag_scores, 1)\n    for i, tag in enumerate(predicted_tags):\n        if tag == targets[i]:\n            total_correct += 1\n    total_total += len(targets)\n            \n# compute the mean loss for the batch\nbatch_correct = total_correct / total_total\nepoch_correct += batch_correct\nepoch_loss = total_loss\nepoch_total = len(dataset_train)\nepoch_total_ct = 1\n\n\nepoch_total_dev = 0\ntotal_loss_dev = 0.0\ntotal_correct_dev = 0\ntotal_total_dev = 0\nepoch_correct_dev = 0\nfor sentence, tags in dataset_dev:\n    sentence_in_dev = texts_to_sequences(sentence, word2idx)\n    targets_dev = texts_to_sequences(tags, tag2idx)\n    tag_scores_dev = model(sentence_in_dev)\n    loss_dev = loss_function(tag_scores_dev, targets_dev)\n    total_loss_dev += loss_dev\n    _, predicted_tags_dev = torch.max(tag_scores_dev, 1)\n    for i, tag in enumerate(predicted_tags_dev):\n        if tag == targets_dev[i]:\n            total_correct_dev += 1\n    total_total_dev += len(targets_dev)\n            \n# compute the mean loss for the batch\nbatch_correct_dev = total_correct_dev / total_total_dev\nepoch_correct_dev += batch_correct_dev\nepoch_total_dev += 1\n\n\nepoch_correct_test = 0\nepoch_total_test = 0\nwith torch.no_grad():\n    total_loss_test = 0.0\n    total_correct_test = 0\n    total_total_test = 0\n    for sentence, tags in dataset_test:\n        sentence_in_test = texts_to_sequences(sentence, word2idx)\n        targets_test = texts_to_sequences(tags, tag2idx)\n        tag_scores_test = model(sentence_in_test)\n        loss_test = loss_function(tag_scores_test, targets_test)\n        total_loss_test += loss_test\n        _, predicted_tags_test = torch.max(tag_scores_test, 1)\n        y_pred = np.concatenate((y_pred, predicted_tags_test.flatten()))\n        y_true = np.concatenate((y_true, targets_test.flatten()))\n        for i, tag in enumerate(predicted_tags_test):\n            if tag == targets_test[i]:\n                total_correct_test += 1\n        total_total_test += len(targets_test)\n            \n    # compute the mean loss for the batch\n    batch_correct_test = total_correct_test / total_total_test\n    epoch_correct_test += batch_correct_test\n    epoch_total_test += 1\n    \ny_true = y_true.tolist()\ny_pred = y_pred.tolist()\nprint(\"loss: %1.5f, accuracy: %1.5f, dev_accuracy: %1.5f, test_accuracy: %1.5f\" % (epoch_loss/epoch_total, epoch_correct/epoch_total_ct, epoch_correct_dev/epoch_total_dev, epoch_correct_test/epoch_total_test))\nprint(classification_report(y_true, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:48:28.319786Z","iopub.execute_input":"2023-03-09T10:48:28.320241Z","iopub.status.idle":"2023-03-09T10:48:42.561468Z","shell.execute_reply.started":"2023-03-09T10:48:28.320201Z","shell.execute_reply":"2023-03-09T10:48:42.560242Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"loss: 0.04639, accuracy: 0.98461, dev_accuracy: 0.97998, test_accuracy: 0.98419\n              precision    recall  f1-score   support\n\n        PRON       0.98      0.99      0.99       392\n         AUX       0.97      0.97      0.97       256\n         DET       1.00      0.99      0.99       512\n        NOUN       0.99      0.99      0.99      1166\n         ADP       0.99      1.00      1.00      1434\n       PROPN       0.98      1.00      0.99      1567\n        VERB       0.98      0.96      0.97       629\n         NUM       0.98      0.93      0.95       127\n         ADJ       0.90      0.98      0.94       220\n       CCONJ       1.00      1.00      1.00       109\n         ADV       0.96      0.68      0.80        76\n        PART       0.98      0.96      0.97        56\n        INTJ       1.00      0.97      0.99        36\n\n    accuracy                           0.98      6580\n   macro avg       0.98      0.96      0.96      6580\nweighted avg       0.98      0.98      0.98      6580\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_sentence = \"i would like the cheapest flight from pittsburgh to atlanta leaving april twenty fifth and returning may sixth\".lower().split()\n\ninputs = texts_to_sequences(test_sentence, word2idx)\ninputs = inputs\ntag_scores = model(inputs)\nprint(tag_scores)\n\n_, predicted_tags = torch.max(tag_scores, 1)\nprint('\\n')\nprint('Predicted tags: \\n',predicted_tags)","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:50:14.495980Z","iopub.execute_input":"2023-03-09T10:50:14.496503Z","iopub.status.idle":"2023-03-09T10:50:14.513972Z","shell.execute_reply.started":"2023-03-09T10:50:14.496458Z","shell.execute_reply":"2023-03-09T10:50:14.512208Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"tensor([[-2.9523e-03, -7.3551e+00, -8.9117e+00, -1.0804e+01, -1.2331e+01,\n         -7.3577e+00, -9.9349e+00, -1.2093e+01, -1.4044e+01, -1.3396e+01,\n         -7.5347e+00, -1.1623e+01, -7.0023e+00, -1.3190e+01],\n        [-8.0075e+00, -2.6613e-03, -9.5596e+00, -1.1533e+01, -1.1196e+01,\n         -1.3398e+01, -6.5412e+00, -1.3542e+01, -9.1066e+00, -1.0986e+01,\n         -7.8230e+00, -9.5238e+00, -8.6485e+00, -1.1680e+01],\n        [-8.1536e+00, -5.3886e+00, -5.4901e+00, -8.5146e+00, -2.7891e+00,\n         -8.9043e+00, -2.1837e-01, -6.7483e+00, -2.3235e+00, -7.5716e+00,\n         -4.7644e+00, -4.0952e+00, -8.1907e+00, -8.0983e+00],\n        [-8.9511e+00, -1.0336e+01, -7.7670e-04, -1.2844e+01, -9.8011e+00,\n         -1.1416e+01, -9.0694e+00, -1.1343e+01, -8.4495e+00, -1.3816e+01,\n         -9.5002e+00, -9.0583e+00, -1.1497e+01, -1.3171e+01],\n        [-1.3329e+01, -1.0100e+01, -8.4905e+00, -7.4045e+00, -1.0031e+01,\n         -9.5345e+00, -8.5299e+00, -8.9448e+00, -2.4207e-03, -9.7423e+00,\n         -6.9063e+00, -1.0116e+01, -1.1430e+01, -1.2119e+01],\n        [-1.4078e+01, -1.4508e+01, -1.4494e+01, -1.0877e-03, -1.0858e+01,\n         -7.7501e+00, -8.5816e+00, -9.2417e+00, -7.9800e+00, -1.3153e+01,\n         -1.2888e+01, -1.4309e+01, -1.2599e+01, -1.5185e+01],\n        [-1.4076e+01, -1.3756e+01, -1.0377e+01, -1.0664e+01, -8.1137e-04,\n         -1.3695e+01, -1.0489e+01, -1.1192e+01, -7.3992e+00, -1.2670e+01,\n         -1.0615e+01, -9.5358e+00, -1.7430e+01, -1.5205e+01],\n        [-9.5260e+00, -1.3956e+01, -1.5313e+01, -7.4556e+00, -9.1980e+00,\n         -1.3004e-03, -1.0153e+01, -8.0162e+00, -1.3715e+01, -9.4931e+00,\n         -1.1666e+01, -1.0396e+01, -9.7014e+00, -1.4891e+01],\n        [-1.1393e+01, -1.3230e+01, -9.2417e+00, -1.5021e+01, -1.2127e-03,\n         -9.2862e+00, -1.0126e+01, -9.0615e+00, -9.8759e+00, -9.1517e+00,\n         -1.0385e+01, -7.3196e+00, -1.4075e+01, -1.3401e+01],\n        [-1.2908e+01, -1.7446e+01, -1.6230e+01, -1.0964e+01, -1.2622e+01,\n         -7.6410e-05, -1.4013e+01, -1.0296e+01, -1.6630e+01, -1.2462e+01,\n         -1.6257e+01, -1.1434e+01, -1.2496e+01, -1.6742e+01],\n        [-1.1810e+01, -1.1375e+01, -7.9592e+00, -8.8211e+00, -9.9669e+00,\n         -1.0048e+01, -9.0129e-04, -9.3284e+00, -9.4349e+00, -1.1671e+01,\n         -1.0151e+01, -1.0703e+01, -9.8091e+00, -1.3960e+01],\n        [-1.3005e+01, -1.4700e+01, -1.3620e+01, -1.4473e-03, -1.1373e+01,\n         -6.7833e+00, -1.1547e+01, -8.5614e+00, -9.4420e+00, -1.2617e+01,\n         -1.1754e+01, -1.5295e+01, -1.1949e+01, -1.5618e+01],\n        [-1.2233e+01, -1.4239e+01, -1.2003e+01, -7.8270e+00, -1.0016e+01,\n         -8.4412e+00, -1.2027e+01, -1.1148e-03, -8.3038e+00, -9.3054e+00,\n         -9.5123e+00, -1.1159e+01, -1.1577e+01, -1.3644e+01],\n        [-1.2726e+01, -8.6518e+00, -7.8310e+00, -4.9514e+00, -9.0149e+00,\n         -9.7078e+00, -8.9653e+00, -4.4107e+00, -2.7002e-02, -7.5177e+00,\n         -5.1398e+00, -9.5436e+00, -9.8563e+00, -1.1652e+01],\n        [-1.1922e+01, -1.1237e+01, -1.2478e+01, -8.6070e+00, -9.8334e+00,\n         -7.0599e+00, -1.0165e+01, -4.8081e+00, -6.5426e+00, -1.2420e-02,\n         -7.9035e+00, -7.6808e+00, -7.2326e+00, -1.0503e+01],\n        [-7.8135e+00, -6.7555e+00, -6.6617e+00, -4.6293e+00, -9.0602e+00,\n         -7.1788e+00, -3.4025e-02, -5.0703e+00, -5.5735e+00, -6.6103e+00,\n         -6.4880e+00, -9.2469e+00, -4.9849e+00, -9.4082e+00],\n        [-7.2743e+00, -2.4540e+00, -8.3903e+00, -1.6164e-01, -8.7197e+00,\n         -6.3624e+00, -3.9405e+00, -7.4184e+00, -3.6954e+00, -6.6512e+00,\n         -4.7781e+00, -8.3876e+00, -5.1873e+00, -9.2873e+00],\n        [-1.2872e+01, -9.5414e+00, -7.8678e+00, -5.8901e+00, -9.2931e+00,\n         -1.2591e+01, -6.3993e+00, -8.1814e+00, -7.2831e-03, -1.0434e+01,\n         -6.2616e+00, -1.0911e+01, -1.0333e+01, -1.1953e+01]],\n       grad_fn=<LogSoftmaxBackward0>)\n\n\nPredicted tags: \n tensor([0, 1, 6, 2, 8, 3, 4, 5, 4, 5, 6, 3, 7, 8, 9, 6, 3, 8])\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(idx2tag)\nct = 0\nfor i in predicted_tags:\n    print(test_sentence[ct], end=\"\\t\")\n    print(idx2tag[i.item()])\n    ct += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-09T10:50:14.820586Z","iopub.execute_input":"2023-03-09T10:50:14.821057Z","iopub.status.idle":"2023-03-09T10:50:14.830423Z","shell.execute_reply.started":"2023-03-09T10:50:14.821016Z","shell.execute_reply":"2023-03-09T10:50:14.828969Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"i\tPRON\nwould\tAUX\nlike\tVERB\nthe\tDET\ncheapest\tADJ\nflight\tNOUN\nfrom\tADP\npittsburgh\tPROPN\nto\tADP\natlanta\tPROPN\nleaving\tVERB\napril\tNOUN\ntwenty\tNUM\nfifth\tADJ\nand\tCCONJ\nreturning\tVERB\nmay\tNOUN\nsixth\tADJ\n","output_type":"stream"}]}]}